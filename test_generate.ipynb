{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "from mingpt.model import GPT\n",
    "from mingpt.trainer import Trainer\n",
    "from mingpt.utils import set_seed, setup_logging, CfgNode as CN\n",
    "set_seed(3407)\n",
    "\n",
    "from chargpt import get_config, CharDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 447013 characters, 79 unique.\n",
      "number of parameters: 2.70M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = os.path.join('./out/chargpt', \"model.pt\")\n",
    "\n",
    "config = get_config()\n",
    "setup_logging(config)\n",
    "set_seed(config.system.seed)\n",
    "\n",
    "# construct the training dataset\n",
    "text = open('data/gpt_train.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(config.data, text)\n",
    "\n",
    "# construct the model\n",
    "config.model.vocab_size = train_dataset.get_vocab_size()\n",
    "config.model.block_size = train_dataset.get_block_size()\n",
    "model = GPT(config.model)\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, context):\n",
    "    x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(\"cpu\")\n",
    "    y = model.generate(x, 25, temperature=1.0, do_sample=True, top_k=1)[0]\n",
    "    completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "    return completion.split(\"\\n\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mollembaum-en'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example generation\n",
    "generate(model, \"Mollembaum-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance on test set\n",
    "pairs = []\n",
    "with open(\"data/gpt_test.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines: \n",
    "        sg, pl = line.strip(\"\\n\").split(\":\")\n",
    "        pairs.append((sg, pl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 0 out of 100 or 0.0\n",
      "Incorrect: 100\n",
      "SG: Urfassung, PL: Urfassungen, PRED: en\n",
      "SG: Küchenreibe, PL: Küchenreiben, PRED: en\n",
      "SG: Regelung, PL: Regelungen, PRED: en\n",
      "SG: Volksverhetzung, PL: Volksverhetzungen, PRED: enen\n",
      "SG: Locher, PL: Locher, PRED: en\n",
      "SG: Mannheimer, PL: Mannheimer, PRED: en\n",
      "SG: Gasplanet, PL: Gasplaneten, PRED: en\n",
      "SG: Etappe, PL: Etappen, PRED: enen\n",
      "SG: Malaysierin, PL: Malaysierinnen, PRED: en\n",
      "SG: Doktorfisch, PL: Doktorfische, PRED: en\n",
      "SG: Ölförderung, PL: Ölförderungen, PRED: en\n",
      "SG: Theorbe, PL: Theorben, PRED: enen\n",
      "SG: Baskenmütze, PL: Baskenmützen, PRED: en\n",
      "SG: Landschaft, PL: Landschaften, PRED: en\n",
      "SG: Patientin, PL: Patientinnen, PRED: en\n",
      "SG: Thiophosphat, PL: Thiophosphate, PRED: en\n",
      "SG: Abgasklappe, PL: Abgasklappen, PRED: en\n",
      "SG: Antike, PL: Antiken, PRED: en\n",
      "SG: Code, PL: Codes, PRED: en\n",
      "SG: Reling, PL: Relings, PRED: n\n",
      "SG: Lederschlaufe, PL: Lederschlaufen, PRED: en\n",
      "SG: Mundschenk, PL: Mundschenke, PRED: \n",
      "SG: Summierung, PL: Summierungen, PRED: en\n",
      "SG: Engländer, PL: Engländer, PRED: en\n",
      "SG: Außwirkung, PL: Außwirkungen, PRED: en\n",
      "SG: Englein, PL: Englein, PRED: enen\n",
      "SG: Zeltstadt, PL: Zeltstädte, PRED: en\n",
      "SG: Benutzerkonto, PL: Benutzerkonten, PRED: en\n",
      "SG: Gütesiegel, PL: Gütesiegel, PRED: en\n",
      "SG: Gebäck, PL: Gebäcke, PRED: enen\n",
      "SG: Scheltwort, PL: Scheltwörter, PRED: en\n",
      "SG: Fegefeuer, PL: Fegefeuer, PRED: en\n",
      "SG: Kopulativkompositum, PL: Kopulativkompositen, PRED: en\n",
      "SG: Bügelverschluss, PL: Bügelverschlüsse, PRED: en\n",
      "SG: Nanobakterium, PL: Nanobakterien, PRED: en\n",
      "SG: Revolverheldin, PL: Revolverheldinnen, PRED: en\n",
      "SG: Stutzer, PL: Stutzer, PRED: enen\n",
      "SG: Reißzwecke, PL: Reißzwecken, PRED: enen\n",
      "SG: Kaiman, PL: Kaimane, PRED: nenen\n",
      "SG: Wasserstoffmolekül, PL: Wasserstoffmoleküle, PRED: en\n",
      "SG: Herzklappe, PL: Herzklappen, PRED: en\n",
      "SG: Essstörung, PL: Essstörungen, PRED: en\n",
      "SG: Marodeur, PL: Marodeure, PRED: en\n",
      "SG: Depositenbank, PL: Depositenbanken, PRED: en\n",
      "SG: Eierlikör, PL: Eierliköre, PRED: en\n",
      "SG: Anabolikum, PL: Anabolika, PRED: en\n",
      "SG: Büchse, PL: Büchsen, PRED: enen\n",
      "SG: Isolani, PL: Isolanis, PRED: en\n",
      "SG: Science-Fiction-Buch, PL: Science-Fiction-Bücher, PRED: en\n",
      "SG: Schulbibliothek, PL: Schulbibliotheken, PRED: en\n",
      "SG: Ypecaharalle, PL: Ypecaharallen, PRED: en\n",
      "SG: Däumchen, PL: Däumchen, PRED: en\n",
      "SG: Kontrollwahn, PL: Kontrollwahne, PRED: enen\n",
      "SG: Bürde, PL: Bürden, PRED: en\n",
      "SG: Scherenfernrohr, PL: Scherenfernrohre, PRED: en\n",
      "SG: Arschkriecher, PL: Arschkriecher, PRED: en\n",
      "SG: Suite, PL: Suiten, PRED: en\n",
      "SG: Schweißverfahren, PL: Schweißverfahren, PRED: en\n",
      "SG: Ergreifung, PL: Ergreifungen, PRED: enen\n",
      "SG: Backenbart, PL: Backenbärte, PRED: enenen\n",
      "SG: Junkie, PL: Junkies, PRED: en\n",
      "SG: Stimmregister, PL: Stimmregister, PRED: en\n",
      "SG: Jobaussicht, PL: Jobaussichten, PRED: en\n",
      "SG: Abfertigungsgebäude, PL: Abfertigungsgebäude, PRED: en\n",
      "SG: Bienenwachs, PL: Bienenwachse, PRED: en\n",
      "SG: Appellmacher, PL: Appellmacher, PRED: en\n",
      "SG: Beiname, PL: Beinamen, PRED: enen\n",
      "SG: Berechnung, PL: Berechnungen, PRED: en\n",
      "SG: Prothese, PL: Prothesen, PRED: en\n",
      "SG: Bläschen, PL: Bläschen, PRED: en\n",
      "SG: Versendung, PL: Versendungen, PRED: en\n",
      "SG: Wohnviertel, PL: Wohnviertel, PRED: en\n",
      "SG: Puffe, PL: Puffen, PRED: enen\n",
      "SG: Immunreaktion, PL: Immunreaktionen, PRED: en\n",
      "SG: Beugung, PL: Beugungen, PRED: en\n",
      "SG: Abbiegung, PL: Abbiegungen, PRED: en\n",
      "SG: Rettungsinsel, PL: Rettungsinseln, PRED: enen\n",
      "SG: Gewässer, PL: Gewässer, PRED: en\n",
      "SG: Einverständniserklärung, PL: Einverständniserklärungen, PRED: en\n",
      "SG: Ersatzfrau, PL: Ersatzfrauen, PRED: en\n",
      "SG: Reaktionsfähigkeit, PL: Reaktionsfähigkeiten, PRED: en\n",
      "SG: Bückling, PL: Bücklinge, PRED: en\n",
      "SG: Gewölbe, PL: Gewölbe, PRED: enen\n",
      "SG: Hardwareentwickler, PL: Hardwareentwickler, PRED: en\n",
      "SG: Fallschirm, PL: Fallschirme, PRED: en\n",
      "SG: Kat, PL: Kats, PRED: en\n",
      "SG: Pumpe, PL: Pumpen, PRED: enen\n",
      "SG: Kaff, PL: Käffer, PRED: en\n",
      "SG: Zeilenabstand, PL: Zeilenabstände, PRED: en\n",
      "SG: Federchen, PL: Federchen, PRED: en\n",
      "SG: Ausbau, PL: Ausbaue, PRED: enen\n",
      "SG: Müllmann, PL: Müllmänner, PRED: en\n",
      "SG: Urlaub, PL: Urlaube, PRED: enen\n",
      "SG: Milchmädchen, PL: Milchmädchen, PRED: enen\n",
      "SG: Wundrand, PL: Wundränder, PRED: en\n",
      "SG: Himmelszelt, PL: Himmelszelte, PRED: en\n",
      "SG: Vielflächner, PL: Vielflächner, PRED: en\n",
      "SG: Hamster, PL: Hamster, PRED: enen\n",
      "SG: Leistungssport, PL: Leistungssporte, PRED: en\n",
      "SG: Höhenkrankheit, PL: Höhenkrankheiten, PRED: enen\n"
     ]
    }
   ],
   "source": [
    "def find_suffix(singular, plural): \n",
    "    last_char = singular[-1]\n",
    "    suffix = []\n",
    "    for i in range(len(plural)-1, -1, -1):\n",
    "        if plural[i] == last_char:\n",
    "            break\n",
    "        else:\n",
    "            suffix.append(plural[i])\n",
    "    return \"\".join(suffix[::-1])\n",
    "\n",
    "test_set = pairs[:100] # TODO remove\n",
    "N = len(test_set)\n",
    "correct = 0\n",
    "incorrect = []\n",
    "for pair in test_set:\n",
    "    sg, pl = pair\n",
    "    context = sg+\":\"\n",
    "    prediction = generate(model, context)\n",
    "    prediction = find_suffix(context, prediction)\n",
    "    if prediction == pl:\n",
    "        correct += 1\n",
    "    else: \n",
    "        incorrect.append((sg, pl, prediction))\n",
    "\n",
    "print(f\"Correct: {correct} out of {N} or {correct/N}\")\n",
    "print(f\"Incorrect: {len(incorrect)}\")\n",
    "for sg, pl, prediction in incorrect:\n",
    "    print(f\"SG: {sg}, PL: {pl}, PRED: {prediction}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
